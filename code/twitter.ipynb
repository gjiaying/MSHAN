{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.models import load_model\n",
    "from keras.layers import TimeDistributed, Input, Layer\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential,Model,Input\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import concatenate\n",
    "import keras\n",
    "from keras import initializers\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from keras import backend as K\n",
    "import math\n",
    "from spektral.layers import GCNConv\n",
    "from spektral.models.gcn import GCN\n",
    "#from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "import keras\n",
    "#nltk.download('stopwords')\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "#embed = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "EMBEDDING_DIMENSION = 512\n",
    "WINDOWS = 5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(company, date):\n",
    "    str = company + '/' + date\n",
    "    filepath = os.getcwd() + '/tweet/preprocessed/' + str\n",
    "    list = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            list.append(data['text'])\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(os.getcwd()+'/price/alltrainings.csv')\n",
    "X_test = pd.read_csv(os.getcwd()+'/price/alltestings.csv')\n",
    "X_val = pd.read_csv(os.getcwd()+'/price/allvalidations.csv')\n",
    "y = X['label']\n",
    "y_test = X_test['label']\n",
    "y_val = X_val['label']\n",
    "y = y[WINDOWS:]\n",
    "y_val = y_val[WINDOWS:]\n",
    "y_test = y_test[WINDOWS:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list = preprocessing('AAPL', '2014-01-01')\n",
    "tokenization = [w for w in list[0] if not w in stop_words]\n",
    "input_array = np.array(embed(list[0]))\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(input_array,maxlen=128,padding='post')\n",
    "print(embed(list[0]))\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(tweet):\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    #tweet = tweet.replace(\"#\", \"\").replace(\"_\", \"\").replace(\"$\", \"\") #Remove hashtag sign but keep the text\n",
    "    tweet = tweet.replace(\"rt\", \"\").replace(\"AT _ USER\", \"\").replace(\"URL\", \"\")\n",
    "    words = tweet.split()\n",
    "    wordsnew = tweet.split()\n",
    "    for i in range(len(words)):       \n",
    "        if '$' == words[i]:\n",
    "         \n",
    "            wordsnew = wordsnew[:i+1] + wordsnew[i + 2:]\n",
    "    \n",
    "    tweet = TreebankWordDetokenizer().detokenize(wordsnew)\n",
    "    tweet = tweet.replace(\"-\",\"\").replace(\"$\",\"\")\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    filepath_com = os.getcwd() + '/price/test_sample/'\n",
    "    company = os.listdir(filepath_com)\n",
    "\n",
    "    total_list = []\n",
    "    for i in range(len(company[:-1])):   #len(company[:-1])\n",
    "        filepath_date = os.getcwd() + '/tweet/preprocessed/' + str(company[i][:-4])\n",
    "        X = pd.read_csv(os.getcwd()+'/price/' + path + str(company[i]))\n",
    "        date = os.listdir(filepath_date)\n",
    "        \n",
    "        for j in range(len(date)): #len(date)-1\n",
    "            if date[j] in X['date'].values.tolist() and date[j][:-4] is not '.ipynb_checkpo':\n",
    "                #print(date[j])\n",
    "                list_oneday = []\n",
    "                list_oneday_score = []\n",
    "                list = preprocessing(company[i][:-4], date[j])\n",
    "                for k in range(len(list)):\n",
    "                    sentence = Sentence(TreebankWordDetokenizer().detokenize(list[k]))\n",
    "                    #print(sentence)\n",
    "                    #print(cleaner(str(sentence)))\n",
    "                    classifier.predict(sentence)\n",
    "                    \n",
    "                    list_oneday_score.append(sentence.labels[0].score)\n",
    "                    oneday = (embed(list[k]))\n",
    "                    \n",
    "                    if oneday.shape[0] >= MAX_LENGTH:\n",
    "                        list_oneday.append(oneday[0:MAX_LENGTH])\n",
    "                    else:\n",
    "                        one_as_vector = tf.reshape(oneday,[-1])\n",
    "                        zero_paddings = tf.zeros([MAX_LENGTH*EMBEDDING_DIMENSION]-tf.shape(one_as_vector),dtype=oneday.dtype)\n",
    "                        oneday_padded = tf.concat([one_as_vector, zero_paddings], 0)\n",
    "                        results = tf.reshape(oneday_padded, [MAX_LENGTH, EMBEDDING_DIMENSION])\n",
    "   \n",
    "           \n",
    "            \n",
    "                        list_oneday.append(results)\n",
    "                \n",
    "                list_oneday_score = np.array(list_oneday_score)\n",
    "                list_oneday_score = list_oneday_score/sum(list_oneday_score)\n",
    "                res = list_oneday * list_oneday_score[:, np.newaxis, np.newaxis]\n",
    "                #list_oneday = list_oneday*list_oneday_score\n",
    "                list_oneday = tf.convert_to_tensor(res)\n",
    "                #print(list_oneday)\n",
    "                \n",
    "               \n",
    "                list_oneday = np.mean(list_oneday,axis=0)\n",
    "                list_oneday = tf.convert_to_tensor(list_oneday)\n",
    "                \n",
    "                total_list.append(list_oneday)\n",
    "        #print(len(total_list))\n",
    "    total_list = tf.convert_to_tensor(total_list)\n",
    "    #gru = tf.keras.layers.GRU(64)\n",
    "    #total_list = (gru(total_list))\n",
    "    print(total_list.shape)\n",
    "    #np.save('embeddings.npy', embed(list[0]))\n",
    "    #all_embeddings = np.load('embeddings.npy')\n",
    "    #print(torch.FloatTensor(np.asarray(all_embeddings)))\n",
    "    return total_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = get_data('train_sample/')\n",
    "validation_data = get_data('val_sample/')\n",
    "testing_data = get_data('test_sample/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_input=[]\n",
    "for i in range(len(training_data)-WINDOWS):\n",
    "  xtrain_input.append(training_data[i:i+WINDOWS,:])\n",
    "xtrain_input=np.array(xtrain_input)\n",
    "print(xtrain_input.shape)\n",
    "xtrain_input = xtrain_input.reshape(15222, 5, -1)\n",
    "\n",
    "xval_input=[]\n",
    "for i in range(len(validation_data)-WINDOWS):\n",
    "  xval_input.append(validation_data[i:i+WINDOWS,:])\n",
    "xval_input=np.array(xval_input)\n",
    "print(xval_input.shape)\n",
    "xval_input = xval_input.reshape(1654, 5, -1)\n",
    "\n",
    "xtest_input=[]\n",
    "for i in range(len(testing_data)-WINDOWS):\n",
    "  xtest_input.append(testing_data[i:i+WINDOWS,:])\n",
    "xtest_input=np.array(xtest_input)\n",
    "print(xtest_input.shape)\n",
    "xtest_input = xtest_input.reshape(2513, 5, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = keras.optimizers.RMSprop(\n",
    "    learning_rate=3e-1,\n",
    "    rho=0.9,\n",
    "    momentum=0.1,\n",
    "    epsilon=1e-07,\n",
    "    centered=False,\n",
    "    name=\"RMSprop\",\n",
    ")\n",
    "print(xtrain_input.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "    #return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def mcc_m(y_true, y_pred):\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    return num / K.sqrt(den + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = TCN(nb_filters=16, return_sequences=True)(inputs)\n",
    "  #  x = Bidirectional(GRU(64, input_shape=input_shape, return_sequences=True))(inputs)\n",
    "  #  x = Dropout(0.3)(x)\n",
    "    x = Dense(16,kernel_initializer=\"uniform\",activation='relu')(x)\n",
    "    for i in range(3):\n",
    "        x1 = TCN(nb_filters=16, return_sequences=True)(inputs)\n",
    "        x1 = Dense(16,kernel_initializer=\"uniform\",activation='relu')(x1)\n",
    "        x = concatenate([x,x1],axis=-1)\n",
    "\n",
    "  \n",
    "        \n",
    " #   x = Bidirectional(GRU(32, return_sequences=True))(x)\n",
    " #   x = Dropout(0.3)(x)\n",
    "  #  x = Bidirectional(GRU(16, return_sequences=True))(x)\n",
    "  #  x = Dropout(0.3)(x)\n",
    "  #  x = SeqSelfAttention(attention_activation='relu')(x)\n",
    "    \n",
    "    #x = Bidirectional(GRU(16, return_sequences=False))(inputs)\n",
    "   # x = Dropout(0.3)(x)\n",
    "   \n",
    "  #  x = Dense(32,kernel_initializer=\"uniform\",activation='relu')(inputs)\n",
    "   # x = Dense(8,kernel_initializer=\"uniform\",activation='relu')(x)\n",
    "    x = Dense(1,kernel_initializer=\"uniform\",activation='relu')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs = x)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    " \n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    bce_loss = bce(y_true, y_pred)\n",
    "    mse_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    loss = 0.5*bce_loss + 0.5*mse_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(input_shape=xtrain_input.shape[1:])\n",
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy', f1_m, mcc_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    # keras.callbacks.EarlyStopping(patience=2),\n",
    "    keras.callbacks.ModelCheckpoint(filepath='modellstm.{epoch:02d}.h5'),\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "history=model.fit(\n",
    "    xtrain_input,\n",
    "    y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=50,\n",
    "    validation_data=(xval_input, y_val),\n",
    "    verbose=1,\n",
    "    callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(xtest_input, y_test, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_hist = []\n",
    "n = 1\n",
    "for i in range(50):\n",
    "  k = str(n).zfill(2)\n",
    "  n += 1\n",
    "  name = 'modellstm.' + k +'.h5'\n",
    "  model11 = load_model(name, custom_objects={'SeqSelfAttention':SeqSelfAttention.get_custom_objects()['SeqSelfAttention'],'TCN':TCN,'f1_m':f1_m,'mcc_m':mcc_m})\n",
    "  model11.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy', f1_m, mcc_m])\n",
    "  loss,acc,f1,mcc = model11.evaluate(xtest_input,y_test,batch_size=BATCH_SIZE)\n",
    "  acc_hist.append(acc)\n",
    "  print(acc)\n",
    "    \n",
    "print(max(acc_hist))\n",
    "print(acc_hist.index(max(acc_hist))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'modellstm.26.h5'\n",
    "model11 = load_model(name, custom_objects={'SeqSelfAttention':SeqSelfAttention.get_custom_objects()['SeqSelfAttention'],'TCN':TCN,'f1_m':f1_m,'mcc_m':mcc_m})\n",
    "model11.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy', f1_m, mcc_m])\n",
    "loss,acc,f1,mcc = model11.evaluate(xtest_input,y_test,batch_size=BATCH_SIZE)\n",
    "print(acc)\n",
    "print(f1)\n",
    "print(mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_3rd_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[0].output])\n",
    "layer_output = get_3rd_layer_output(xtest_input)[0]\n",
    "layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs6604]",
   "language": "python",
   "name": "conda-env-cs6604-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
